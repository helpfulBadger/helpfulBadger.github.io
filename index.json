[{
    "title": "Signing HTTP Requests",
    "date": "October 7, 2020",
    "description": "Learn how to use Envoy & OPA to sign and validate HTTP Requests",
    "body": "Photo by Vanna Phon on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 09 \u0026mdash; Learn how to use Envoy \u0026amp; OPA to sign and validate HTTP Requests This is the 9th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 9 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Introduction In this example we are going to show how we can use Envoy and Open Policy Agent to sign HTTP requests as we come through a front proxy and validate these signatures on a 2nd Envoy / Open Policy Agent instance that is dedicated to the endpoint that we are protecting.\nThe diagram below shows system and request flow that we will be building.\nSigning and validating signatures on a request is very useful in large environments where a request may traverse a number of different application layers, proxies and other components that are owned by a variety of teams. Some of those components may even be controlled by 3rd parties.\n A digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents. A valid digital signature, where the prerequisites are satisfied, gives a recipient very strong reason to believe that the message was created by a known sender (authentication), and that the message was not altered in transit (integrity).[1]\n  Digital signatures are a standard element of most cryptographic protocol suites, and are commonly used for software distribution, financial transactions, contract management software, and in other cases where it is important to detect forgery or tampering.\n  Source: Wikipedia\n Digital signatures can be a great tool to prevent internal fraud. In large corporations it is common to utilize centralized log aggregation tools. These tools are often open to all employees / workforce users that have valid credentials. This is a great practice for transparency, understanding dependencies, troubleshooting etc. However, sometimes:\n Novice engineers or engineers that are not used to working in large corporations may log sensitive data elements such as access tokens or other headers that can be used to forge a malicious request that moves money, purchases a product etc. 3rd Parties such as a cloud provider that operates an API gateway, a lambda product or any other product that acts as a proxy to your code will have access to the entire request / response stream. They have their own logs. A malicious engineer at the cloud provider can log and alter or forge any request / response flowing through their products whenever a TLS connection is terminated by their product.  Walking through the docker-compose file Link to compose file\nEnvoy Instances\nAs shown in the highlighted rows (lines 6 \u0026amp; 13), each Envoy proxy is built from dockerfiles located in their own directories. Additionally, since we have traces turned on, we mapped local volumes (lines 8 \u0026amp; 15) into the Envoy containers to capture and expose the captured requests and responses. For more information on how to set this up, there is a getting started guide for it.\n  1version: \u0026#34;3.7\u0026#34; 2services: 3 4 front-envoy: 5 build: 6\u0026gt; context: ./front-proxy 7 volumes: 8\u0026gt; - ./tmp/front:/tmp/any 9 ... 10 11 service1: 12 build: 13\u0026gt; context: ./service1 14 volumes: 15\u0026gt; - ./tmp/service1:/tmp/any 16 ...  Open Policy Agent Instances\nAs shown in the highlighted rows (lines 3 \u0026amp; 15), just like the Envoy instances, each Open Policy Agent instance is built from dockerfiles located in their own directories. The policy files (lines 11 \u0026amp; 23) are consistently named but have different content and purposes.\n 1 sign: 2 build: 3# context: ./sign 4 ... 5 command: 6 - \u0026#34;run\u0026#34; 7 - \u0026#34;--log-level=debug\u0026#34; 8 - \u0026#34;--server\u0026#34; 9 - \u0026#34;--set=plugins.envoy_ext_authz_grpc.addr=:9191\u0026#34; 10 - \u0026#34;--set=decision_logs.console=true\u0026#34; 11# - \u0026#34;/config/policy.rego\u0026#34; 12 13 verify: 14 build: 15# context: ./verify 16 ... 17 command: 18 - \u0026#34;run\u0026#34; 19 - \u0026#34;--log-level=debug\u0026#34; 20 - \u0026#34;--server\u0026#34; 21 - \u0026#34;--set=plugins.envoy_ext_authz_grpc.addr=:9191\u0026#34; 22 - \u0026#34;--set=decision_logs.console=true\u0026#34; 23# - \u0026#34;/config/policy.rego\u0026#34; 24  Walking through the signing policy Link to signing policy\nAs a request flows through a complex set of systems, a lot of headers are injected and / or removed on various hops. These headers might be used for:\n Routing Injecting or updating tracing headers Injecting or removing other headers that are meaningful to the proxies  Our signature needs to survive these transformations. We need to make sure we declare what should not change and then only include that in the signature. To help use communicate these pieces of information to the recipient, we will be creating a signed JSON web token to hold our HTTP request signature.\nLet\u0026rsquo;s walk through some highlights of the REGO Policy file.\n We need a private signing key to ensure that the signature has not been tampered with. This is declared on line 5 but would be retrieved from a key management system in a production deployment.  The headers that we want to remain unchanged throughout the process are declared on line 8. These include some other JWS tokens. These tokens prove the identity of the user, the application originating the request, the subject / entity that is being acting on behalf of (if applicable). Additionally, for troubleshooting purposes, we have bound the session-id and request-id into the signature to ensure traceability outside of any open tracing solutions.   Calculating a hash of the headers  There are 3 steps required to calculate the hash of the headers.  The first step on line 11 is to create a new object that only contains the headers of interest from the original request. The object.filter() rego built in function does that for us. It takes 2 parameters. The headers we want to include in the new object and the original object that we need to pull them from. We convert the object to a JSON string on line 12, json.marshal. This REGO built-in function consistently orders the keys in the resulting output string. If this was not the case, then we would not be able to use REGO for this. Finally, we calculate the hash of the headers with the crypto.sha256() built in function.     1package envoy.authz 2 3import input.attributes.request.http as http_request 4 5signingKey = { ... } // Private Key for creating the signature 6 7// Headers that we would like included in the signature 8criticalHeaders = [\u0026#34;actor-token\u0026#34;, \u0026#34;app-token\u0026#34;, \u0026#34;subject-token\u0026#34;, \u0026#34;session-id\u0026#34;, \u0026#34;request-id\u0026#34;] 9 10// We calculate the header hash by ... 11filteredHeaders = object.filter( http_request.headers, criticalHeaders )\t12headerString = json.marshal( filteredHeaders ) 13headerHash = crypto.sha256( headerString ) \nCalculating a Hash of the Body  We need to ensure the body is initialized before calculating a hash for it. Line 16 sets the body to an empty string. If http_request.body on line 18 is missing then the default value (empty string) is assigned to the body variable. Then we can calculate the hash of the body using the crypto.sha256( body ) built in function on line 21   14// Ensure the request body is initialized and fetch the request\u0026#39;s body if one is present 15default body = \u0026#34;\u0026#34; 16body = b { 17 b := http_request.body 18} 19 20// Then calculate the hash for the request body 21bodyHash = crypto.sha256( body ) \nCommunicating our signature Now that we have the hash of our headers and body, we need to gather all of the information that we want to put into our signature. We will lock all of this information together by binding it into a JWS token. We will use some standard claims and create some custom claims as well.\n 22requestDigest = { 23 \u0026#34;iss\u0026#34;: \u0026#34;apigateway.example.com\u0026#34;, 24 \u0026#34;aud\u0026#34;: [ \u0026#34;protected-api.example.com\u0026#34;], 25 \u0026#34;host\u0026#34;: http_request.host, 26 \u0026#34;method\u0026#34;: http_request.method, 27 \u0026#34;path\u0026#34;: http_request.path, 28 \u0026#34;created\u0026#34;: time.now_ns(), 29 \u0026#34;headers\u0026#34;: criticalHeaders, 30 \u0026#34;headerDigest\u0026#34;: headerHash, 31 \u0026#34;bodyDigest\u0026#34;: bodyHash 32} 33 34digestHeader = io.jwt.encode_sign({ \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34;, \u0026#34;alg\u0026#34;: \u0026#34;RS256\u0026#34; }, requestDigest, signingKey )   The system sending the request identifies itself as the issuer (line 23). We can also specify the recipient of the request (line 24). Since the JWS is signed, we can simply copy the host, method, \u0026amp; resource path (lines 25-27). A timestamp (line 28) informs the recipient of how long the token has been in transit. The recipient can determine how long to honor a request. The recipient needs to know which headers (line 29) were included in the header hash and of course it needs to know the hash (line 30). The recipient also needs to know what the hash of the body is (line 31). Finally, we create our JWS by passing our JWT object, our signing key and some parameters to specify the signing algorthim into the io.jwt.encode_sign() built in function.  Handing off to Envoy to add the signature to the request With the signature calculated, all that remains to be done is to attach it to the outbound request.\n As we learned previously, when using the external authorization feature, we can tell Envoy to insert or update headers by setting the header name and its value inside a headers object. In our case, we don\u0026rsquo;t want to interfere with any other authorization mechanisms that are in use in the environment. So, instead of putting our signature in the authorization header, we will create a Digest header (line 38). Our OPA policy wasn\u0026rsquo;t evaluating any real authorization rules. So we will always set the allowed variable to true (line 36). We could implement this as a service mesh sidecar to sign all outbound requests on behalf of an application. In a front proxy use case, we most likely would have run other rules before signing the request and forwarding it to it\u0026rsquo;s ultimate destination.   35allow = { 36 \u0026#34;allowed\u0026#34;: true, # Outbound requests are always allowed. This policy simply signs the request 37 \u0026#34;headers\u0026#34;: { 38 \u0026#34;Digest\u0026#34;: digestHeader 39 } 40} \nTests for the signing policy Link to signing policy tests\nThe signatures should work equally well whether each of the fields we are signing is present and populated, present but empty or missing entirely. This will ensure that if a field is optional, the signature will still be created and if a fake value is inserted it can still be detected.\nFor each of these use cases we the precalculated the hashes for a given input.\n The fullyPopulated variable (line 5) is used to simulate the input received from Envoy. The fullyPopulatedJws variable (line 6) is the precalculated JWS token that represents our signature. We can directly refer to variables in our main REGO policy from their associated tests. On lines 11, 15 and 19 we test to see if those variables contain our expected results. This same pattern is repeated for the \u0026lsquo;empty\u0026rsquo; and \u0026lsquo;missing\u0026rsquo; use cases.   1package envoy.authz 2 3fullyPopulated = { ... } 4fullyPopulatedJws = { 5 \u0026#34;bodyDigest\u0026#34;: \u0026#34;60009cec5b535270a0b8389cea67c894fae9549c17b2ceef8f824cde3a10b14e\u0026#34;, 6 \u0026#34;headerDigest\u0026#34;: \u0026#34;87329ba8383ff39b40746aa22e8d4ee58facc5ac470cac410efb6e549f7574fb\u0026#34;, 7} 8 9# Fully populated request 10test_fully_populated_req_bodyHash_matches { 11 bodyHash == fullyPopulatedJws.bodyDigest with input as fullyPopulated 12} 13 14test_fully_populated_req_headerHash_matches { 15 headerHash == fullyPopulatedJws.headerDigest with input as fullyPopulated 16} 17 18test_fully_populated_req_allowed { 19 allow.allowed with input as fullyPopulated 20} \nValidating the signature upon Receipt Link to signature verification policy\nExtracting the signature from the request The first part of the policy:\n Extracts the digest from the incoming request header Validates the JWS Digest token Places the contents of the validated token in a variable for other rules   1package envoy.authz 2 3import input.attributes.request.http as http_request 4import input.attributes.request.http.headers[\u0026#34;digest\u0026#34;] as digest 5 6jwks = `{...}` 7 8verified_digest = v { 9 [isValid, _, payload ] := io.jwt.decode_verify( digest, 10 { 11 \u0026#34;cert\u0026#34;: jwks, 12 \u0026#34;aud\u0026#34;: \u0026#34;apigateway.example.com\u0026#34; // \u0026lt;-- Required since the token contains an `aud` claim in the payload 13 }) 14 v := { 15 \u0026#34;isValid\u0026#34;: isValid, 16 \u0026#34;payload\u0026#34;: payload 17 } 18}   Line 4: Extracts the request\u0026rsquo;s signature token from the Digest header Line 8: If validated, sets a variable to hold the decoded token payload Line 9: Decodes and validates the token using the io.jwt.decode_verify() built-in function Line 11: Provides the decode function the public keys it needs to validate the signature Line 12: We must provide an expected audience claim when the token contains an audience claim. The audience claim is an array. If any member of that array matches the supplied audience then that validation rule will pass. Line 14: Assigns the decoded token to named properties in an object that in turn is assigned to the verified_digest  Comparing the request to the validated token Now we can do the important part and compare the values in the request with what was preserved in the JWS token.\n 19headersMatch { 20 headerHash == verified_digest.payload.headerDigest 21} 22bodiesMatch { 23 bodyHash == verified_digest.payload.bodyDigest 24} 25hostsMatch { 26 http_request.host == verified_digest.payload.host 27} 28methodsMatch { 29 http_request.method == verified_digest.payload.method 30} 31pathsMatch { 32 http_request.path == verified_digest.payload.path 33}   Line 20: The list of critical headers from the JWS token is used to filter the request\u0026rsquo;s headers and calculate a hash using the same statements that were used in the signing process (not shown). If they match then headersMatch is set to true. Line 23: The request\u0026rsquo;s body is extracted and used to calculate a hash using the same statements that were used in the signing process (not shown). If they match then bodiesMatch is set to true. Lines 26, 29 \u0026amp; 32: Since these values are captured directly in the token, no special processing is required. They are simply compared to see if they have been altered.  Checking Request Recency We don\u0026rsquo;t want to leave an unbounded amount of time for a request to be processed. The next section of the policy checks to see if the request was initiated recently enough.\n 34requestDuration = time.now_ns() - verified_digest.payload.created 35 36withinRecencyWindow { 37 requestDuration \u0026lt; 34159642955430000 38}   Line 34: uses the built-in function time.now_ns() to calculate the time passed since the request was initiated. Line 37: Uses the calculated request duration and compares it to our business rule  Generating Rule Failure Messages An this side of the connection our messages need 2 conditions to properly know which rules failed.\n 39messages[ msg ]{ 40 verified_digest.isValid 41\tnot headersMatch 42 msg := { 43 \u0026#34;id\u0026#34; : \u0026#34;7\u0026#34;, 44 \u0026#34;priority\u0026#34; : \u0026#34;5\u0026#34;, 45 \u0026#34;message\u0026#34; : \u0026#34;Critical request headers do not match signature\u0026#34; 46 } 47}   Line 40: If the digest token is simply missing or corrupt, we don\u0026rsquo;t want to return messages for every single rule that failed. If the signature token is missing entirely, the issue isn\u0026rsquo;t really that the headers don\u0026rsquo;t match. The real issue is that the signature token is missing. So, this guard rule makes sure that we at least had a valid signature token before we check to see if the headers didn\u0026rsquo;t match. Line 41: If the headers don\u0026rsquo;t match then we return message indicating that. Numerous other similar rules are in the full policy file (not shown here) testing for similar conditions.  Calculating the final decision on the validity of the request Lines 49 through 55 below calculate the final decision on the validity of the request. When we name our rules intuitively, the decision logic is pretty intuitive as well. In this case the methods, paths, hosts, headers and bodies must all\n 48default decision = false 49decision { 50 methodsMatch 51 pathsMatch 52 hostsMatch 53 headersMatch 54 bodiesMatch 55 withinRecencyWindow 56} 57 58default headerValue = \u0026#34;false\u0026#34; 59headerValue = h { 60 decision 61 h := \u0026#34;true\u0026#34; 62} 63 64allow = { 65 \u0026#34;allowed\u0026#34;: decision, 66 \u0026#34;headers\u0026#34;: { 67 \u0026#34;Valid-Request\u0026#34;: headerValue 68 }, 69 \u0026#34;body\u0026#34; : json.marshal({ \u0026#34;Authorization-Failures\u0026#34;: messages }) 70}  Line 69: OPA\u0026rsquo;s built-in function json.marshal() converts all of the rule failures into a string for sending back to the caller. The other statements should be familiar from our previous discussion of Envoy\u0026rsquo;s external authorization contract. So, we won\u0026rsquo;t break that down again.\nThe signature verification policy tests are more complicated than the tests for the signing process due to increased complexity of the policy.\nCongratulations We have completed our example to demonstrate how to sign and validate HTTP requests. This capability is very powerful and effective and protecting the integrity of transactions. The best part about this approach is that this powerful capability can be added to any application in your portfolio without requiring code changes to every system.\n",
    "ref": "/blog/envoy_opa_9_sign_verify/"
  },{
    "title": "Configuring Envoy Logs, Taps and Traces",
    "date": "September 10, 2020",
    "description": "Learn how to configure Envoy's access logs, taps for capturing full requests & responses and traces",
    "body": "Photo by Kazuky Akayashi on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 08 \u0026mdash; Learn how to configure Envoy\u0026rsquo;s access logs, taps for capturing full requests \u0026amp; responses and traces for capturing end to end flow This is the 8th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 8 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Introduction In this example we are going to use both a Front Proxy deployment and a service mesh deployment to centralize log configuration, capture full requests and responses with taps and to inject trace information. Logs and taps can be done transparently without the knowledge nor cooperation from our applications. However, for traces there cooperation from our app is required to forward the trace headers to the next link in the chain.\nThe diagram below shows what we will be building.\nThe Envoy instances throughout our network will be streaming logs, taps and traces on behalf of the applications involved in the request flow.\nLet\u0026rsquo;s Start with Configuring Our Logs Envoy gives you the ability configure what it logs as a request goes though the proxy. Envoy\u0026rsquo;s web site has documentation for access log configuration. There are a few things to be aware of:\n Each request / response header must be individually configured to make it to the logs. I haven\u0026rsquo;t found a log-all-headers capability. This is can be a good thing because sensitive information doesn\u0026rsquo;t get logged without a conscious decision to log it. On the other side of the coin, if you are using multiple trace providers, then you will miss trace headers from these other solutions until:  You are even aware that they are in use You update the configuration for every node There is no ability to run analytics from the time they started being used. You will only be able to go back to the time you became aware of these new headers and updated your configurations.   The property name match_config that we use in this article is entering deprecation when Envoy 1.16 comes out. I\u0026rsquo;ll will update this post to the new property name once 1.16 is released. We will be using Elastic Common Schema in this example to the extent that we can with Envoy 1.15\u0026rsquo;s configuration limitations. Also coming with Envoy 1.16 will be nested JSON support. Once released I\u0026rsquo;ll update the example to switch from the dot notation used here to nested JSON.  Read through this snippet of envoy configuration. The full configuration files for our 3 envoy instances are located here:\n Front Proxy\u0026rsquo;s Configuration Service 1\u0026rsquo;s Envoy Configuration Service 2\u0026rsquo;s Envoy Configuration  Elastic common schema was introduced to make it easier to analyze logs across applications. This article on the elastic.co web site is a good read and explains it in more detail. The reference for elastic common schema provides names for many typical deployment environments, and cloud environments etc.\nBelow is the abbreviation envoy.yaml configuration that shows how to specify our logging configuration. The explanation of the configuration follows.\n 1static_resources: 2 listeners: 3 ... 4 - address: 5 filter_chains: 6 - filters: 7 - name: envoy.filters.network.http_connection_manager 8 typed_config: 9 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 10 generate_request_id: true 11 ... 12 route_config: 13 http_filters: 14 ... 15\u0026gt; access_log: 16 - name: envoy.access_loggers.file 17 typed_config: 18\u0026gt; \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog 19\u0026gt; path: \u0026#34;/dev/stdout\u0026#34; 20\u0026gt; typed_json_format:  21 \u0026#34;@timestamp\u0026#34;: \u0026#34;%START_TIME%\u0026#34; 22\u0026gt; client.address: \u0026#34;%DOWNSTREAM_REMOTE_ADDRESS%\u0026#34; 23 client.local.address: \u0026#34;%DOWNSTREAM_LOCAL_ADDRESS%\u0026#34; 24\u0026gt; envoy.route.name: \u0026#34;%ROUTE_NAME%\u0026#34; 25 envoy.upstream.cluster: \u0026#34;%UPSTREAM_CLUSTER%\u0026#34; 26 host.hostname: \u0026#34;%HOSTNAME%\u0026#34; 27 http.request.body.bytes: \u0026#34;%BYTES_RECEIVED%\u0026#34; 28 http.request.duration: \u0026#34;%DURATION%\u0026#34; 29 http.request.headers.accept: \u0026#34;%REQ(ACCEPT)%\u0026#34; 30 http.request.headers.authority: \u0026#34;%REQ(:AUTHORITY)%\u0026#34; 31\u0026gt; http.request.headers.id: \u0026#34;%REQ(X-REQUEST-ID)%\u0026#34; 32 http.request.headers.x_forwarded_for: \u0026#34;%REQ(X-FORWARDED-FOR)%\u0026#34; 33 http.request.headers.x_forwarded_proto: \u0026#34;%REQ(X-FORWARDED-PROTO)%\u0026#34; 34 http.request.headers.x_b3_traceid: \u0026#34;%REQ(X-B3-TRACEID)%\u0026#34; 35 http.request.headers.x_b3_parentspanid: \u0026#34;%REQ(X-B3-PARENTSPANID)%\u0026#34; 36 http.request.headers.x_b3_spanid: \u0026#34;%REQ(X-B3-SPANID)%\u0026#34; 37 http.request.headers.x_b3_sampled: \u0026#34;%REQ(X-B3-SAMPLED)%\u0026#34; 38 http.request.method: \u0026#34;%REQ(:METHOD)%\u0026#34; 39 http.response.body.bytes: \u0026#34;%BYTES_SENT%\u0026#34; 40\u0026gt; service.name: \u0026#34;envoy\u0026#34; 41 service.version: \u0026#34;1.16\u0026#34; 42 ... 43 clusters: 44admin:  The access_log configuration section is part of the HTTP Connection Manager Configuration and at the same nesting level as the route_config and http_filters sections.\n We use a typed config type.googleapis.com/envoy.extensions.access_loggers.file.v3.FileAccessLog in version 3 of the configuration API The file path is set to standard out to so that log aggregation can be managed by docker. The typed_json_format property is what allows us to create logs in JSON Lines format Envoy gives us template strings to insert into the configuration that it will replace with the request specific values at run time. client.address: \u0026quot;%DOWNSTREAM_REMOTE_ADDRESS%\u0026quot;. In these examples, we have pretty much used every available template string. The client section of elastic common schema holds everything that we know about the client application that is originating the request. We use dot notation here since nested JSON support is not available in Envoy 1.15 Elastic Common Schema does not have standard names for everything. So we created a section for unique Envoy properties envoy.route.name: \u0026quot;%ROUTE_NAME%\u0026quot; The configuration language has macros that enable us to do lookups. In this example we lookup the X-Request-Id header and put it in the http.request.headers.id field http.request.headers.id: \u0026quot;%REQ(X-REQUEST-ID)%\u0026quot; We can insert static values into the logs for hardcoding labels such as which service the log entry is for etc. service.name: \u0026quot;envoy\u0026quot;  Now Let\u0026rsquo;s Tackle Taps Taps let us capture full requests and responses. More details are for how the tap feature works is located in the Envoy Documentation. There are 2 ways that we can capture taps:\n Statically configured with a local filesystem directory as the target for output. Envoy will store the request and response for each request in a separate file. The output directory must be specified with a trailing slash. Otherwise the files will not be written. The good news here is that you don\u0026rsquo;t need log rotate. You can have a separate process scoop up each file, send the data to a log aggregator and then delete the file. The second method for getting taps is by sending a configuration update to the admin port with a selection criteria and then holding the connection open while waiting for taps to stream in over the network. This would obviously be the preferred approach for a production environment.  Below is abbreviated structure of our Envoy.yaml config file. The highlighted section is the tap configuration. Taps are simply another available HTTP filter.\n We have match configuration that allows us to selectively target what we tap and what we don\u0026rsquo;t. The output configuration section is where we specify where the taps should be sent. For the file_per_tap sink, remember to add the trailing slash to the path otherwise no files will be written. Additionally, the file path can\u0026rsquo;t be set to standard out due to the expectation that new files will be opened under the subdirectory that is specified.   1static_resources: 2 listeners: 3 ... 4 - address: 5 filter_chains: 6 - filters: 7 - name: envoy.filters.network.http_connection_manager 8 typed_config: 9 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 10 route_config: 11 http_filters: 12 - name: envoy.filters.http.tap 13 typed_config: 14 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.tap.v3.Tap 15 common_config: 16 static_config: 17 match_config: 18 any_match: true 19 output_config: 20 sinks: 21 - file_per_tap: 22 path_prefix: /tmp/any/ 23 - name: envoy.filters.http.router 24 typed_config: {} 25 access_log: 26 ... 27 clusters: 28 ... 29admin: 30...   NOTE: The tap filter is experimental and is currently under active development. There is currently a very limited set of match conditions, output configuration, output sinks, etc. Capabilities will be expanded over time and the configuration structures are likely to change.\n Enabling Traces The overview documentation for Envoy\u0026rsquo;s Tracing features succinctly describes Envoy\u0026rsquo;s tracing capabilities very well. So, I have included a snippet here as a lead in to our configuration.\n Distributed tracing allows developers to obtain visualizations of call flows in large service oriented architectures. It can be invaluable in understanding serialization, parallelism, and sources of latency. Envoy supports three features related to system wide tracing:\n Request ID generation: Envoy will generate UUIDs when needed and populate the x-request-id HTTP header. Applications can forward the x-request-id header for unified logging as well as tracing. The behavior can be configured on per HTTP connection manager basis using an extension. Client trace ID joining: The x-client-trace-id header can be used to join untrusted request IDs to the trusted internal x-request-id. External trace service integration: Envoy supports pluggable external trace visualization providers, that are divided into two subgroups:  External tracers which are part of the Envoy code base, like LightStep, Zipkin or any Zipkin compatible backends (e.g. Jaeger), and Datadog. External tracers which come as a third party plugin, like Instana.     Tracing options are exploding and as such expect a lot of new capabilities to be added to Envoy. There are a great deal of configuration options already. We are demonstrating the simplest possible tracing solution. Therefore, configuration below leverages the built in support for Zipkin compatible backends (jaegar).\nIn lines 11 -19 below, the tracing configuration is standard part of the version 3 connection manager. The tracing property contains a provider.\n We have set up zipkin as the output format with jaegar as the configured destination. HTTP and gRPC are both supported. The example uses HTTP protocol for transmitting the traces. Our trace configuration refers to a cluster that we have named jaeger. So, we need to add a cluster configuration for it in the clusters section of the configuration file. Lines 28 through 40 points Envoy to our all-in-one jaeger container and port 9411 on that container.   1static_resources: 2 listeners: 3 - address: 4 ... 5 filter_chains: 6 - filters: 7 - name: envoy.filters.network.http_connection_manager 8 typed_config: 9 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 10 generate_request_id: true 11\u0026gt; tracing: 12 provider: 13 name: envoy.tracers.zipkin 14 typed_config: 15 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.config.trace.v2.ZipkinConfig 16 collector_cluster: jaeger 17 collector_endpoint: \u0026#34;/api/v2/spans\u0026#34; 18 shared_span_context: false 19 collector_endpoint_version: HTTP_JSON 20 codec_type: auto 21 route_config: 22 ... 23 http_filters: 24 ... 25 clusters: 26 - name: service1 27 ... 28\u0026gt; - name: jaeger 29 connect_timeout: 1s 30 type: strict_dns 31 lb_policy: round_robin 32 load_assignment: 33 cluster_name: jaeger 34 endpoints: 35 - lb_endpoints: 36 - endpoint: 37 address: 38 socket_address: 39 address: jaeger 40 port_value: 9411 41admin: 42 ...  Running the Solution There is a bash script in the 08_log_taps_traces directory that demonstrates the completed example. Just run ./demonstrate_log_tap_and_trace.sh The script does the following:\n It cleans up any left over files from a previous run. Creates temporary directories to hold the taps and log files that we will capture. Starts up 3 envoy instances, 2 services, and jaeger as drawn in our diagram at the top of this article. Shows the containers running for validation that there were no failures. Sends a request through the Envoy front proxy and onwards through the service mesh. Displays the custom access logs captured by each of the 3 Envoy proxies.    These logs are captured in JSON Lines format. They are run through a tool called jq (running in a docker container). JQ pretty prints each log line and sorts the keys to make it easier to see the data groupings. Service 1 has 2 log lines since it captures the request as it goes into service 1 and it in turn calls service 2.   Shows the taps captured in all 3 envoy instances. These taps show the full request and response details. The request and response bodies are base64 encoded in the taps in case they are binary. So, the script decodes each body that is available and shows it to the user. It uses the open command to open Jaeger in a web browser. If it doesn\u0026rsquo;t work on your operating system simply navigate to http://localhost:16686/ on your own. Finally it shuts down the environment. It leaves all of the logs and taps in the tmp directory for further inspection.  Let\u0026rsquo;s take a look at our traces Jaeger Landing Page\nThe Jaeger landing page displays a summary of recent traces. It also enables filtering and selection of traces.\nCall Graph\nThe Jaeger call graph page shows the entire call tree. Ours is pretty simple. It also shows response time details and where this trace fits withing the norms.\nTrace Details\nThe Jaeger trace details page shows all of the tags that were captured at each leg of the request\u0026rsquo;s journey.\nCongratulations We have completed a tour of another Envoy feature. These observability features will come in handy as our system becomes more complex. Additionally, Envoy is a very powerful tool for intercepting our traffic in a legacy environment. This can give all of our legacy applications consistently formatted data across our entire environment. This super-power can be incredibly critical in an environment with a lot of older applications that don\u0026rsquo;t have active development, limited engineering knowledge and obsolete technology stacks.\n",
    "ref": "/blog/envoy_opa_8_logs_taps_and_traces/"
  },{
    "title": "Putting It All Together with Composite Authorization",
    "date": "September 7, 2020",
    "description": "Learn how to Implement Application Specific Authorization Rules",
    "body": "Photo by Thom Bradley on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 07 \u0026mdash; Learn how to Implement Application Specific Authorization Rules This is the 7th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 7 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Introduction In this example we are going to simulate an ecommerce company called Example.com that has a published set of APIs and multiple client applications. Each client application has:\n Different access rights to each published API Different operations they are allowed to perform on the APIs that it has access to Different types of users that are allowed to use the application  There are a lot of other rules that we will eventually be interested in implementing. However, this set of rules is complex enough to demonstrate how we can put together what we have learned so far into a little authorization system.\nExample.com\u0026rsquo;s APIs Here are Example.com\u0026rsquo;s published APIs.\n 1/api/customer/\u0026#39;{id}\u0026#39; 2/api/customer/\u0026#39;{id}\u0026#39;/account/\u0026#39;{id}\u0026#39; 3/api/customer/\u0026#39;{id}\u0026#39;/messages/\u0026#39;{id}\u0026#39; 4/api/customer/\u0026#39;{id}\u0026#39;/order/\u0026#39;{id}\u0026#39; 5/api/customer/\u0026#39;{id}\u0026#39;/paymentcard/\u0026#39;{id}\u0026#39; 6/api/featureFlags 7/api/order/\u0026#39;{id}\u0026#39; 8/api/order/\u0026#39;{id}\u0026#39;/payment/\u0026#39;{id}\u0026#39; 9/api/product/\u0026#39;{id}\u0026#39; 10/api/shipment/\u0026#39;{id}\u0026#39;   The customer API, /api/customer/*, allows users manage customer profiles in our customer system of record. The accounts API, /api/customer/*/account/*, allows users to manage accounts for a specific customer in the system of record for accounts. The messages API, /api/customer/*/messages/*, allows users to send and receive messages related to a specific customer in the messaging system. The customer\u0026rsquo;s order API, /api/customer/*/order/*, allows users to manage a customer\u0026rsquo;s orders. The customer\u0026rsquo;s payment card API, /api/customer/*/paymentcard/*, allows users to manage a customer\u0026rsquo;s debit or credit cards. The feature flag API, /api/featureFlags, allows an application to retrieve the feature flags for the app e.g. which features are turned on or off and any customer specific features that are available or not. The order API, /api/order/*, allows users to get, create, update or delete orders for any customer. The order payments API, /api/order/*/payment/*, allows users to manage payments for any order. No shopping cart API is provided. An order in draft status is the equivalent of a shopping cart. The product API, /api/product/*, allows users to manage the products that are available for purchase. The shipment API, /api/shipment/*, allows users to manage shipment for an order.  API Endpoint Definition As the basis for our security policies we need a data structure that contains all of possible actions that a user and client application can take. For this example, we define a URI pattern and a method being attempted on that URI as an endpoint. The id field uniquely identifies each endpoint and will be used in the process of actually specifying what endpoints an application has access to.\n 1[ 2 {\u0026#34;id\u0026#34;:\u0026#34;001\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer\u0026#34;}, 3 {\u0026#34;id\u0026#34;:\u0026#34;002\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;POST\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer\u0026#34;}, 4 {\u0026#34;id\u0026#34;:\u0026#34;003\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;DELETE\u0026#34;,\u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*\u0026#34;}, 5 {\u0026#34;id\u0026#34;:\u0026#34;004\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*\u0026#34;}, 6 {\u0026#34;id\u0026#34;:\u0026#34;005\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;POST\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*\u0026#34;}, 7 {\u0026#34;id\u0026#34;:\u0026#34;006\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;PUT\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*\u0026#34;}, 8 {\u0026#34;id\u0026#34;:\u0026#34;007\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*/account\u0026#34;}, 9 {\u0026#34;id\u0026#34;:\u0026#34;008\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;POST\u0026#34;, \u0026#34;pattern\u0026#34;:\u0026#34;/api/customer/*/account\u0026#34;}, 10... 11]  Client Application API Contracts The next piece of data that we need to build our example authorization solution is a mapping between each client application and the endponts that it is allowed to access. The data structure below holds that information. The unique ID for each application is a key in this data structure and the value is an array of all of the endpoint IDs that the application has access to.\n 1apiPermissions = { 2 \u0026#34;app_123456\u0026#34;: [ 3 \u0026#34;001\u0026#34;,\u0026#34;004\u0026#34;,\u0026#34;007\u0026#34;,\u0026#34;010\u0026#34;,\u0026#34;012\u0026#34;,\u0026#34;015\u0026#34;,\u0026#34;018\u0026#34;,\u0026#34;021\u0026#34;,\u0026#34;024\u0026#34;,\u0026#34;027\u0026#34;,\u0026#34;031\u0026#34;,\u0026#34;034\u0026#34;,\u0026#34;037\u0026#34;,\u0026#34;040\u0026#34;,\u0026#34;043\u0026#34;,\u0026#34;046\u0026#34;,\u0026#34;049\u0026#34;,\u0026#34;052\u0026#34;,\u0026#34;055\u0026#34; 4 ], 5 \u0026#34;app_000123\u0026#34;:[ 6 \u0026#34;001\u0026#34;, \u0026#34;002\u0026#34;, \u0026#34;003\u0026#34;, \u0026#34;004\u0026#34;, \u0026#34;005\u0026#34;, \u0026#34;006\u0026#34;, \u0026#34;007\u0026#34;, \u0026#34;008\u0026#34;, \u0026#34;009\u0026#34;, \u0026#34;010\u0026#34;, 7 \u0026#34;011\u0026#34;, \u0026#34;012\u0026#34;, \u0026#34;013\u0026#34;, \u0026#34;014\u0026#34;, \u0026#34;015\u0026#34;, \u0026#34;016\u0026#34;, \u0026#34;017\u0026#34;, \u0026#34;018\u0026#34;, \u0026#34;019\u0026#34;, \u0026#34;020\u0026#34;, 8 \u0026#34;021\u0026#34;, \u0026#34;022\u0026#34;, \u0026#34;023\u0026#34;, \u0026#34;024\u0026#34;, \u0026#34;025\u0026#34;, \u0026#34;026\u0026#34;, \u0026#34;027\u0026#34;, \u0026#34;028\u0026#34;, \u0026#34;029\u0026#34;, \u0026#34;030\u0026#34;, 9 \u0026#34;031\u0026#34;, \u0026#34;032\u0026#34;, \u0026#34;033\u0026#34;, \u0026#34;034\u0026#34;, \u0026#34;035\u0026#34;, \u0026#34;036\u0026#34;, \u0026#34;037\u0026#34;, \u0026#34;038\u0026#34;, \u0026#34;039\u0026#34;, \u0026#34;040\u0026#34;, 10 \u0026#34;041\u0026#34;, \u0026#34;042\u0026#34;, \u0026#34;043\u0026#34;, \u0026#34;044\u0026#34;, \u0026#34;045\u0026#34;, \u0026#34;046\u0026#34;, \u0026#34;047\u0026#34;, \u0026#34;048\u0026#34;, \u0026#34;049\u0026#34;, \u0026#34;050\u0026#34;, 11 \u0026#34;051\u0026#34;, \u0026#34;052\u0026#34;, \u0026#34;053\u0026#34;, \u0026#34;054\u0026#34;, \u0026#34;055\u0026#34;, \u0026#34;056\u0026#34;, \u0026#34;057\u0026#34; 12 ] 13}  Authorized End User Identity Providers / JWS Issuers for each Client Application Just for fun and simplicity we want to make sure that a hacker has not found a way to use or simulate using an application that is intended for another type of user. So, the data structure below lists the identity providers that are permitted for each application. So, the data structure below means that app_123456 is only allowed to be used by external customers. app_000123 is intended only for use by employees and contractors of example.com (i.e. the workforce). This is a more powerful application that can take action on behalf of the company and on behalf of any of the company\u0026rsquo;s customer. This is very coarse grained security. In a real application we would also put in a lot of user specific rights and access controls. In a later getting started guide we will show how we can start to layer OPA Policies, getting volatile data from external sources and other techniques to implement more realistic security policies.\n 1idProviderPermissions = { 2 \u0026#34;app_123456\u0026#34;: [ \u0026#34;customerIdentity.example.com\u0026#34; ], 3 \u0026#34;app_000123\u0026#34;: [ \u0026#34;workforceIdentity.example.com\u0026#34; ] 4}  Our Environment In this example we have replaced our HTTPBin container as our ultimate destination with Hoverfly. Hoverfly is a light-weight, super fast API mocking server that is API driven and highly extensible. This is used to simulate all of the APIs that we defined above. The API Mocks are defined in the config.json file in the compose\\hoverfly directory.\nThe other change that we have made is a much more sophisticated and comprehensive set of tests using Newman. Newman is a command line driven postman collection runner. Postman has a testing capability. This feature runs tests that are expressed in Javascript and uses a built in library of test functions. The Postman collection for Getting Started #7 with tests is included in this example in the data directory that\nAuthorization Policies User and system identity information is communicated as follows:\n We have some sort of access token in the Authorization header. What API gateway is in use and how it works is out of scope for this example. Some user authentication system that we are using (out of scope of this article) has produced evidence of authentication in the form of an JWS Identity Token and given this to our client application. The client application places the identity token in the Actor-Token header of the request. The API gateway, the client application itself or another system (out of scope of this article), has securely authenticated the client application and given it a JWS token to assert information in a tamper proof way about our client application.  Stage 1 - Envoy validates JWS Tokens These JWS tokens are validated by Envoy before they are ever sent to OPA. Envoy and OPA Getting Started Guide #5 showed us how to do this. Our Envoy.yaml file for this example has made one small change to the JWT provider configurations. In this example we need to be able to support a variety of different types of users. Since we have 2 different user types / token issuers, we create to different JWT Provider configurations (one for each identity provider). Both of them use the from_headers: parameter to specify the token source as the Actor-Token header.\nBecause of this variability we had to change our logic for token validation and enforcement:\nThe configuration snippet below is how we express this logic:\n The match object specifies that any URI must meet the requirements in the requires object The requires object specifies that a request must have: (a Gateway issued system identity token) AND a user with ( a workforce Identity token OR a Consumer Identity Token )) NOTE: Scroll right as needed. The color coding above matches the translation to YAML configuration below.   1 rules: 2 - match: 3 prefix: / 4 requires: 5 requires_all: # AND 6 requirements: 7\u0026gt; - provider_name: gateway_provider 8 - requires_any: # OR 9 requirements: 10\u0026gt; - provider_name: workforce_provider 11\u0026gt; - provider_name: consumer_provider   The requires_all object specifies that all of the requirements in the requirements array must be true to pass. The requirements array contains a provider_name and a requires_any clause. If there were 3 or 4 elements in the array then all of the requirements would need to be true to pass. In this example we only have 2 requirements and one of those has sub-requirements. The requires_any object contains another requirements array. If any of the requirements in this array are true then the requires_any will also evaluate to true.  Passing Envoy\u0026rsquo;s Dynamic Meta data to OPA We can pass the validated JWS token body to Open Policy Agent by adding a couple of tags to Envoy\u0026rsquo;s configuration. In the token definition section, we add the payload_in_metadata property and give the token a name. In the example below we\u0026rsquo;ve given the token the name actor_token. See below for the configuration snippet.\n 1 workforce_provider: 2 issuer: workforceIdentity.example.com 3 audiences: 4 - apigateway.example.com 5 from_headers: 6 - name: \u0026#34;actor-token\u0026#34; 7 value_prefix: \u0026#34;\u0026#34; 8 forward: true 9\u0026gt; payload_in_metadata: \u0026#34;actor_token\u0026#34; 10 local_jwks: 11 inline_string: \u0026#34;{\\\u0026#34;keys\\\u0026#34;:[...]}\u0026#34; To pass this data to Open Policy Agent, in the envoy.filters.http.ext_authz section, add the array named metadata_context_namespaces. This array specifies the names of the Envoy dynamic metadata namespaces to forward to the OPA Authorization Service. See the configuration change highlighted in blue below.\n 1 - name: envoy.filters.http.ext_authz 2 typed_config: 3 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.http.ext_authz.v3.ExtAuthz 4 failure_mode_allow: false 5 grpc_service: 6 google_grpc: 7 target_uri: opa:9191 8 stat_prefix: ext_authz 9 timeout: 0.5s 10 include_peer_certificate: true 11\u0026gt; metadata_context_namespaces: 12\u0026gt; - envoy.filters.http.jwt_authn 13 - name: envoy.filters.http.router 14 typed_config: {} These changes add some data to the metadata_context. For each dynamic metadata namespace added, it will add it\u0026rsquo;s data under a key that matches the namespace name in the filter_metadata object. For this example it adds each token\u0026rsquo;s payload in the envoy.filters.http.jwt_authn object underneath the fields object. As you can see from the example below, it adds a lot of extra hierarchy via nested objects that express data types to the original object. So, it may not be worth it to have Envoy forward the validated payload. For us configuring Envoy to forward the token and simply reading the token payload directly is less complicated.\nSee the resultant metadata_context below for an example.\n 1 \u0026#34;input\u0026#34;: { 2 \u0026#34;attributes\u0026#34;: { 3 \u0026#34;destination\u0026#34;: { \u0026#34;...\u0026#34;:\u0026#34;...\u0026#34; }, 4 \u0026#34;metadata_context\u0026#34;: { 5\u0026gt; \u0026#34;filter_metadata\u0026#34;: { 6 \u0026#34;envoy.filters.http.jwt_authn\u0026#34;: { 7 \u0026#34;fields\u0026#34;: { 8 \u0026#34;workforce_provider\u0026#34;: { 9 \u0026#34;Kind\u0026#34;: { 10 \u0026#34;StructValue\u0026#34;: { 11 \u0026#34;fields\u0026#34;: { 12 \u0026#34;aud\u0026#34;: { 13 \u0026#34;Kind\u0026#34;: { 14 \u0026#34;ListValue\u0026#34;: { 15 \u0026#34;values\u0026#34;: [ 16 { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;apigateway.example.com\u0026#34;} }, 17 { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;protected-stuff.example.com\u0026#34; } } 18 ] 19 } 20 } 21 }, 22 \u0026#34;auth_time\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;NumberValue\u0026#34;: 1597676718 } }, 23 \u0026#34;azp\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;app_123456\u0026#34; } }, 24 \u0026#34;exp\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;NumberValue\u0026#34;: 2735689600 } }, 25 \u0026#34;iat\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;NumberValue\u0026#34;: 1597676718 } }, 26 \u0026#34;iss\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;workforceIdentity.example.com\u0026#34; } }, 27 \u0026#34;jti\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;mPPdwJ7Jrr2MQzS\u0026#34; } }, 28 \u0026#34;sub\u0026#34;: { \u0026#34;Kind\u0026#34;: { \u0026#34;StringValue\u0026#34;: \u0026#34;workforceIdentity:406319\u0026#34; } } 29 } 30 } 31 } 32 } 33 } 34 } 35 } 36 }, 37 \u0026#34;request\u0026#34;: {\u0026#34;...\u0026#34;: \u0026#34;...\u0026#34;}, 38 \u0026#34;source\u0026#34;: {\u0026#34;...\u0026#34;: \u0026#34;...\u0026#34;} 39 }, 40 \u0026#34;parsed_body\u0026#34;: null, 41 \u0026#34;parsed_path\u0026#34;: [\u0026#34;...\u0026#34;], 42 \u0026#34;parsed_query\u0026#34;: {}, 43 \u0026#34;truncated_body\u0026#34;: false 44 }  Authorization Stage 2 - Open Policy Agent Policies Extracting the JWS Token Payloads Since we also have the configuration setting forward: true, we can simply use OPA\u0026rsquo;s built in capabilities to more easily access the data that we need from the tokens.\n The REGO snippets below show how we pull the tokens that we need out of the actor-token header and app-token header (lines 1 \u0026amp; 6). Since we won\u0026rsquo;t get to this point unless Envoy successfully validated the tokens, we can simply decode the payload. The io.jwt.decode() functions do that for us (lines 3 \u0026amp; 8). We don\u0026rsquo;t need the header and signature values so we tell OPA to discard those with underscores using destructuring assignment [ _, p, _ ] The payload is then stored in the actor and app variables  See below for the REGO code.\n 1import input.attributes.request.http.headers[\u0026#34;actor-token\u0026#34;] as actorToken 2actor = p { 3 [ _, p, _ ] := io.jwt.decode( actorToken )\t4} 5 6import input.attributes.request.http.headers[\u0026#34;app-token\u0026#34;] as appToken 7app = p { 8 [ _, p, _ ] := io.jwt.decode( appToken ) 9}  Making the Authorization Decision Next, we will move over to the punchline, the REGO rule that makes our authorization decision. As we can see from the rule below our rule defaults to false a.k.a. deny access. The next part of the rule says:\n IF the API is permitted for our client app AND the type of user is appropriate for the client app THEN set the decision to true a.k.a. allow the request to flow through.   1default decision = false 2decision { 3 apiPermittedForClient 4 userTypeAppropriateForClient 5}  How do we determine if the API is Permitted for the Client Application? Now lets work backwards to see how we determine if these two conditions are true. Let\u0026rsquo;s look at how apiPermittedForClient is defined. This section determines that the API call is permitted for the client application if the endpoint ID that we are calling is found in the array endpoints that are permitted for the calling app.\n The default value is set to false (line 1). Line 3:  Takes the calling Client App\u0026rsquo;s permissions list apiPermissions[ app.client_id ] and by using the [_] syntax, compares every element in the API permissions array to the called endpointID.     1default apiPermittedForClient = false 2apiPermittedForClient { 3 apiPermissions[ app.client_id ][_] == endpointID 4} \nHow do we determine which API endpoint was called? Determining which endpoint was called with our endpoint rule is a little more complicated:\n 1endpointID = epID { 2 some i 3 endpoints[i].method == http_request.method 4 p := trim_right( http_request.path, \u0026#34;/\u0026#34;) // strip trailing slash if present 5 glob.match( lower(endpoints[i].pattern), [\u0026#34;/\u0026#34;], lower(p) ) 6 epID := endpoints[i].id 7} else = \u0026#34;none\u0026#34;  This series of rules acts as a progressive set of filters that reduces the search space.\nLine 1: endpointID = epID {\n This statement overrides the default REGO behavior of setting rule results to either true, false or undefined. Instead it looks inside the rule body and extracts the epID variable and sets endpointID to that value.  Line 2: some i\n The keyword some is used when we need to match a specific element in one dataset with some other specific value. This line just declares our iterator and names it.  Line 3: endpoints[i].method == http_request.method\n searches for all values for i where the method property of the ith element equals the request method The statement effectively filters out ~75% of our reference data and only keeps the array elements that match our request method.  Line 4: p := trim_right( http_request.path, \u0026quot;/\u0026quot;)\n sets p equal to the request path with any trailing slashes removed. This normalizes our data for comparing against the API reference dta. With the path now normalized, it should work in our REGEX expression search.  Line 5: glob.match(lower(endpoints[i].pattern), [\u0026quot;/\u0026quot;], lower(p))\n takes the lower case pattern from our reference data and the lower case path from our request and looks for any patterns in our reference array that match. Since the statement on line 3 discards ~75% of the reference array to satisfy it\u0026rsquo;s match criteria, the statement on line 5 searches the remaining 25% for any values of i where this statement is true too. Our magic variable i should only represent a single value at this point.  Line 6: epID := endpoints[i].id\n So, now i should be a single value or undefined This statement extracts the endpoint id or leaves epID as undefined.  Line 7: } else = \u0026quot;none\u0026quot;\n if epID is still undefined by this point, it sets epID to the string value of none  How do we determine if the Type of User is Permitted for the Client Application?  1default userTypeAppropriateForClient = false 2userTypeAppropriateForClient { 3 idProviderPermissions[ app.client_id ][_] == actor.iss 4}  Line 1: Now, let\u0026rsquo;s look at how userTypeAppropriateForClient is defined. This line sets its default value to false.\nLine 3: To see if it should override this default value:\n It uses our reference data object, idProviderPermissions By using the client ID from the app token, app.client_id, it narrows down the dataset search The [_] fragment says: for any element in the array, see if one of the equals the issuer that we have in the actor token actor.iss.  If we don\u0026rsquo;t have a match, how do we communicate reject reasons? Whether rejection reasons are returned to the user or not depends on the circumstances and security risks posed by providing this transparency. However, it still makes sense to log the reject reasons. This example shows you how to report back which rules failed and resulted in a denial of access. We do this by defining multiple messages rules.\n 1messages[ msg ]{ 2 not apiPermittedForClient 3 msg := { 4 \u0026#34;id\u0026#34; : \u0026#34;1\u0026#34;, 5 \u0026#34;priority\u0026#34; : \u0026#34;5\u0026#34;, 6 \u0026#34;message\u0026#34; : \u0026#34;The requested API Endpoint is not permitted for the client application\u0026#34; 7 } 8} 9 10messages[ msg ]{ 11 not userTypeAppropriateForClient 12 msg := { 13 \u0026#34;id\u0026#34; : \u0026#34;2\u0026#34;, 14 \u0026#34;priority\u0026#34; : \u0026#34;5\u0026#34;, 15 \u0026#34;message\u0026#34; : \u0026#34;The authenticated user type is not allowed for the client application\u0026#34; 16 } 17}   For each messages rule, we use a single logic statement that tests each authorization rule that we want to create a message for.  If we have multiple rules, then we won\u0026rsquo;t know which reason code to enter. For each authorization rule that did not pass (lines 2 and 11), a new msg object is added to the messages array. Each msg should be statically defined to describe the appropriate failure reason. Inserting a variable that has some possibility of not being defined will cause message insertion to fail.   The phrasing of the statement not apiPermittedForClient, is important. When a rule is evaluated, the result can be one of 3 conditions: true, false or undefined. Using the not keyword ensures that if the result of rule firing is either false or undefined then both conditions will properly cause our error message to be populated. This section will result in a messages array that has either 0, 1 or 2 elements.  Now let\u0026rsquo;s return the results to Envoy The structure of our response is defined by the Envoy external authorization API contract. Envoy is looking for results of the envoy.authz.allow rule. The contract requires:\n A boolean decision allowed A headers map of keys of type string with values that are also a string A body of type string  Note: OPA is a little bit finicky when assigning values. So we must be very careful to make sure that there is no way any of the variables in this contract can end up in an undefined state. If variable assignment attempts to put an undefined value into the object then this rule will fail and the entire result returned to Envoy will be undefined.\n 1allow = response { 2 response := { 3 \u0026#34;allowed\u0026#34;: decision, 4 \u0026#34;headers\u0026#34;: { 5 \u0026#34;X-Authenticated-User\u0026#34;: \u0026#34;true\u0026#34;, 6 \u0026#34;X-Authenticated-App\u0026#34;: \u0026#34;true\u0026#34;, 7 \u0026#34;Endpoint-ID\u0026#34;: endpointID, 8 \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; 9 }, 10 \u0026#34;body\u0026#34; : json.marshal( 11 { 12 \u0026#34;Authorized-User-Type\u0026#34;: userTypeAppropriateForClient, 13 \u0026#34;Authorized-Endpoint\u0026#34;: apiPermittedForClient, 14 \u0026#34;Authorization-Failures\u0026#34; : messages, 15 \u0026#34;Requested-Path\u0026#34;: http_request.path, 16 \u0026#34;Requested-Method\u0026#34;: http_request.method 17 }) 18 } 19}  Line 3:\n Since we have setup a default value for the decision variable, it will always be either true or false and will not cause the response to become undefined.  Line 5 \u0026amp; 6:\n Since Envoy validated our JWS tokens before our policy executed, the User and App tokens will always be valid. So, we can hard coded the value of these headers to the \u0026ldquo;true\u0026rdquo; as a string.  Line 7:\n The endpointID has a default string value of \u0026ldquo;none\u0026rdquo;. Since all of the IDs in our API reference data are already, when there is a match in the reference data, the result here will always be a string.  Line 10:\n The body must be a string. So, we can\u0026rsquo;t return any of the other mock data that we have listed here as an object. So, first we declare the data that we want to return as an anonymous object. Then we immediately pass it to the JSON marshaller. The JSON marshaller can deal with undefined and null values. The JSON marshaller will then always returns a string. With these guarantees, we can be confident that this block will always return a defined result to Envoy  With that explanation behind us, you can read through the entire policy in context on github.\nRunning our Example Through Its Paces We use Postman and Newman to setup the tests for this getting started guide. There are some great tutorials on the Postman site that describe how to write test scripts.\n Writing Pre-request Scripts Writing Test Scripts Test Script Examples Working with Data Files Running test from the command line with Newman  This example is a lot more powerful than the others we have tackled in this series of getting started guides. As such it requires a lot more tests to see if it is functioning correctly. We have a postman collection with 2 types of tests:\n Statically defined tests in the folder named Static-Test-Cases Tests that use iteration data in all of the other folders  Static Test Cases This folder contains one templated request definition for every endpoint. That is 56 requests in total. This collection is simply to show how many endpoints even a simple web site requires to operate. We have created folder level scripts to set up data for our templates.\nFolder level Pre-Request Script\nThe pre-request script simply sets the templated values for the request port and JWS tokens for the User and application. These statically defined values use the \u0026ldquo;super app\u0026rdquo; that has permission for all endpoints. The postman library provides the pm.environment.set command to set values in our template.\n 1pm.environment.set(\u0026#34;port\u0026#34;, \u0026#34;8080\u0026#34;); 2pm.environment.set(\u0026#34;ActorToken\u0026#34;, \u0026#34;...\u0026#34;) 3pm.environment.set(\u0026#34;AppToken\u0026#34;, \u0026#34;...\u0026#34;);  Folder level Test Script\nThe test script simply checks for a successful response.\n 1pm.test(\u0026#34;response should be okay\u0026#34;, function () { 2 pm.response.to.be.success; 3 pm.response.to.not.be.error; 4});  Iteration Data Based Tests Postman / Newman loads each record in the array in our test data file into the template variables in our Postman collection. Each record has the following properties:\n Test case ID The App ID The Actor Token to use The Application token to use The host to send the request to The port on that host to send the request to The method to use in the request The URI to use in the request The pattern that this test exercises The expected result  An example is shown below.\n 1{ 2 \u0026#34;id\u0026#34;: \u0026#34;001\u0026#34;, 3 \u0026#34;App\u0026#34;: \u0026#34;app_000123\u0026#34;, 4 \u0026#34;ActorToken\u0026#34;: \u0026#34;...\u0026#34;, 5 \u0026#34;AppToken\u0026#34;: \u0026#34;...\u0026#34;, 6 \u0026#34;host\u0026#34;: \u0026#34;localhost\u0026#34;, 7 \u0026#34;port\u0026#34;: \u0026#34;8080\u0026#34;, 8 \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;, 9 \u0026#34;uri\u0026#34;: \u0026#34;/api/customer\u0026#34;, 10 \u0026#34;pattern\u0026#34;: \u0026#34;/api/customer\u0026#34;, 11 \u0026#34;expect\u0026#34;: 200 12}  The post execution test only needs one piece of data from the record for our completion test. That is the expect value from the record. We get that using the pm.iterationData.get command.\n 1var expect = pm.iterationData.get(\u0026#34;expect\u0026#34;) 2 3pm.test(\u0026#34;GET response have return status: \u0026#34; + expect, function () { 4 pm.response.to.have.status( Number( expect) ) 5});  There are 4 sets of templates and data files. You can run them all to test our permissions with the following command \u0026hellip;\n./demonstrate_user_and_api_contracts.sh \u0026hellip;in the example\u0026rsquo;s based directory 07_opa_validate_method_uri.\nCongratulations We have just completed our first semi-realistic example. In future lessons we will layer on more features and refactor this example to show a more realistic deployment.\n",
    "ref": "/blog/envoy_opa_7_app_contracts/"
  },{
    "title": "JWS Signature Validation with Envoy",
    "date": "September 7, 2020",
    "description": "Learn how to validate JWS signatures natively with Envoy",
    "body": "Photo by Cytonn Photography on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 06 \u0026mdash; JWS Signature Validation with Envoy This is the 6th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 6 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Introduction One of the HTTP filters available is the JSON Web Token filter. It is lines 14 - 27 highlighted below. You an specify any number of providers. For each provider the developer specifies the desired validation rules. In our case we have 3 tokens that we will be validating. For each we specify:\n The issuers and audiences that must be present. Just like with Open Policy Agent audiences, the requirement is that the specified audience is matches one of the audiences in the array. The from_headers property tells Envoy where to find the JWS The forward property specifies if the token should be forwarded to the protected API or if the header should be removed before forwarding. If the JWS tokens can be misused by the protected API then they should be removed. The local_jwks propery allows you to specify the JSON web keyset in the configuration. Another option is to retrieve them from an HTTP endpoint that hosts the key set.  The configuration below shows the properties we just described on lines 22, 25 and 26.\n 1static_resources: 2 listeners: 3 - address: 4 ... 5 filter_chains: 6 - filters: 7 - name: envoy.filters.network.http_connection_manager 8 typed_config: 9 \u0026#34;@type\u0026#34;: type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager 10 ... 11 route_config: 12 ... 13 http_filters: 14 - name: envoy.filters.http.jwt_authn 15 typed_config: 16 \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.http.jwt_authn.v3.JwtAuthentication\u0026#34; 17 providers: 18 workforce_provider: 19 issuer: workforceIdentity.example.com 20 audiences: 21 - apigateway.example.com 22\u0026gt; from_headers: 23 - name: \u0026#34;actor-token\u0026#34; 24 value_prefix: \u0026#34;\u0026#34; 25\u0026gt; forward: true 26\u0026gt; local_jwks: 27 inline_string: \u0026#34;...\u0026#34;  The section highlighted below is the rules section. It defines under what conditions to look for and validate a JWS. The match section defines what prefixes to look for. The slash will look for JWS tokens on every URI path. We can specify quite a few rules to determine how many and which tokens we require and under what circumstances. The Official JWT Auth documentation specifies several other options on how to craft logic using and, or and any operations as well as other locations where JWS tokens can be located. There isn\u0026rsquo;t as much control over what response to return to clients in the case of a failed authentication. OPA lets the developer change chose the HTTP Status code, include messages in the response body add headers etc. Envoy\u0026rsquo;s built in feature simply returns an HTTP 401 Unauthorized response.\n 28 consumer_provider: 29 ... 30 gateway_provider: 31 ... 32\u0026gt; rules: 33 - match: 34 prefix: / 35 requires: 36 requires_all: 37 requirements: 38 - provider_name: workforce_provider 39 - provider_name: consumer_provider 40 - provider_name: gateway_provider 41 - name: envoy.filters.http.router 42 typed_config: {} 43 clusters: 44 ... 45admin: 46...  To see this capability in action, simply run the demonstrate_envoy_jws_validation.sh script. The output is similar to the completed solution from the previous lesson. The script also dumps the Envoy logs to show the information that you have available to trouble shoot issues. Below is a screen shot of the log statements that show Envoy extracting the tokens, performing the validation and logging the result.\nConclusion In this getting started example, we successfully validated 3 different JWS tokens in a single request and had flexibility to chose where the tokens were pulled from, how many tokens were required and under what conditions those tokens were needed. In our next getting started guide, we will use Open Policy Agent and our identity tokens to make some more sophisticated authorization decisions.\n",
    "ref": "/blog/envoy_opa_6_envoy_jws/"
  },{
    "title": "JWS Token Validation with OPA",
    "date": "September 7, 2020",
    "description": "Learn how to validate JWS tokens with Open Policy Agent",
    "body": "Photo by SkillScouter on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 05 \u0026mdash; JWS Token Validation with OPA This is the 5th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 5 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Introduction This getting started guide will validate 3 different identity tokens that are expressed as Signed JSON web tokens (JWS) for short. This guide assumes knowledge of several IETF RFCs:\n The format for a JWS and standard claims included in it is specified by the IETF in RFC 7515 The signing algorithms for JSON Web tokens is specified by the IETF in RFC 7518 Encryption keys used for signing and validation are specified by the IETF in RFC 7517  There is another tool that was used to create this getting started guide, jose-util:\n JSON Web Token Signing and Encryption is commonly abbreviated as JOSE A command line utility called jose-util is available as part of a Golang JOSE library from Square To install it execute:   go get -u github.com/square/go-jose/jose-util   go install github.com/square/go-jose/jose-util  and make sure your go install directory is in your path    Use of this tool is optional since the tool has alredy been used and the results are already stored in the project.\nSimulated Scenario The scenario being simulated is a customer calling into a company to inquire about an order that they have placed. Various JWS tokens are used as a tamper resistent mechanism to communicate information about these participants through the API request chain.\nThe JSON Web Tokens The keys sub directory contains encryption keys that have been pre-created for you. If you would like to create new keys and have jose-util installed then run the script ./generate_keys.sh\nThe identities sub directory contains simulated identity tokens as JSON files. The script ./create_jws_tokens.sh can be used to sign and verify these tokens.\nProof of Authentication for an External Customer User A customer has called in to a company to enquire about an order that they have placed. The customer was authenticated by going through the Voice Response Unit (VRU) and evidence of that authentication is captured as a JWS token and is placed in the subject-token header. The customer identity provider performed the authentication. This token saves repeated database lookups and is used to authorize what the customer can do and has access to.\n The iss field contains the customer identity provider as the issuer. The profileRefID is a customer claim that can be used to get the customer\u0026rsquo;s preferences. The customerRefID is a custom claim that can be used to get details about the customer. The consentID is a custom claim that can be used to get details about what the customer has consented to and opted out from. The remaining claims are standard claims that contain information about the token issuance, use and authentication methods performed.  The full token is below:\n 1{ 2 \u0026#34;iss\u0026#34;: \u0026#34;customerIdentity.example.com\u0026#34;, 3 \u0026#34;sub\u0026#34;: \u0026#34;customerIdentity:DBm4FBclSD261G2\u0026#34;, 4 \u0026#34;profileRefID\u0026#34;: \u0026#34;34983598sfkh9w8798798d\u0026#34;, 5 \u0026#34;customerRefID\u0026#34;: \u0026#34;DBm4FBclSD261G2\u0026#34;, 6 \u0026#34;consentID\u0026#34;: \u0026#34;4378afd4f\u0026#34;, 7 \u0026#34;aud\u0026#34;: [ \u0026#34;apigateway.example.com\u0026#34;, \u0026#34;protected-stuff.example.com\u0026#34;], 8 \u0026#34;azp\u0026#34;: \u0026#34;app_123456\u0026#34;, 9 \u0026#34;exp\u0026#34;: 2735689600, 10 \u0026#34;iat\u0026#34;: 1597676718, 11 \u0026#34;auth_time\u0026#34;: 1597676718, 12 \u0026#34;jti\u0026#34;: \u0026#34;o8T2hxraeFzIQ6p\u0026#34;, 13 \u0026#34;nonce\u0026#34;: \u0026#34;n-0S6_WzA2Mj\u0026#34;, 14 \u0026#34;acr\u0026#34;: \u0026#34;urn:mace:incommon:iap:silver\u0026#34;, 15 \u0026#34;amr\u0026#34;: [ 16 \u0026#34;face\u0026#34;, 17 \u0026#34;fpt\u0026#34;, 18 \u0026#34;geo\u0026#34;, 19 \u0026#34;mfa\u0026#34; 20 ], 21 \u0026#34;vot\u0026#34;: \u0026#34;P1.Cc.Ac\u0026#34;, 22 \u0026#34;vtm\u0026#34;: \u0026#34;https://example.org/vot-trust-framework\u0026#34; 23}  Proof of Authentication for a Workforce User The call center agent was authenticated by the application she / he is using. Evidence of that authentication is captured as a JWS token is placed in the actor-token header. The workforce identity provider performed the authentication. This token saves repeated database lookups and is used to authorize what an agent can do on their own behalf as well as what an agent can do on the customer\u0026rsquo;s behalf.\n The iss field contains the workforce identity provider as the issuer. The remaining claims are standard claims that contain information about the token issuance, and use.  The full token is below:\n 1{ 2 \u0026#34;iss\u0026#34;: \u0026#34;workforceIdentity.example.com\u0026#34;, 3 \u0026#34;sub\u0026#34;: \u0026#34;workforceIdentity:406319\u0026#34;, 4 \u0026#34;aud\u0026#34;: [ \u0026#34;apigateway.example.com\u0026#34;, \u0026#34;protected-stuff.example.com\u0026#34;], 5 \u0026#34;azp\u0026#34;: \u0026#34;app_123456\u0026#34;, 6 \u0026#34;exp\u0026#34;: 2735689600, 7 \u0026#34;iat\u0026#34;: 1597676718, 8 \u0026#34;auth_time\u0026#34;: 1597676718, 9 \u0026#34;jti\u0026#34;: \u0026#34;mPPdwJ7Jrr2MQzS\u0026#34; 10}  Proof of Authentication for an Application The application that the call center agent is using was authenticated by the API gateway. Proof of authentication and application details are placed in the app-token header. This stateless token saves repeated database lookups and is used for application level authorizations.\n The iss field contains the API gateway as the issuer. The client_id The remaining claims are standard claims that contain information about the token issuance, and use.  The full token is below:\n 1{ 2 \u0026#34;iss\u0026#34;: \u0026#34;apigateway.example.com\u0026#34;, 3 \u0026#34;sub\u0026#34;: \u0026#34;apigateway:app_123456\u0026#34;, 4 \u0026#34;aud\u0026#34;: [ \u0026#34;apigateway.example.com\u0026#34;, \u0026#34;protected-stuff.example.com\u0026#34;], 5 \u0026#34;azp\u0026#34;: \u0026#34;app_123456\u0026#34;, 6 \u0026#34;client_id\u0026#34;: \u0026#34;app_123456\u0026#34;, 7 \u0026#34;exp\u0026#34;: 2735689600, 8 \u0026#34;iat\u0026#34;: 1597676718, 9 \u0026#34;auth_time\u0026#34;: 1597676718, 10 \u0026#34;jti\u0026#34;: \u0026#34;XcOpQe2vqTq1kny\u0026#34;, 11 \u0026#34;grant\u0026#34;: \u0026#34;client_credentials\u0026#34;, 12 \u0026#34;owningLegalEntity\u0026#34;: \u0026#34;Example Co.\u0026#34;, 13 \u0026#34;scopes\u0026#34;: [ 14 \u0026#34;rewards:read\u0026#34;, 15 \u0026#34;rewards:redeem\u0026#34; 16 ], 17 \u0026#34;kid\u0026#34;: \u0026#34;APIGW-ES256\u0026#34; 18}  Open Policy Agent Examples In the policy examples directory, there are 2 different REGO policies:\n jws_examples_v1.rego - which shows three diffent commands to process a JWS policy.rego - which shows our final policy  jws_examples_v1.rego  1verify_example = { \u0026#34;isValid\u0026#34;: isValid} { 2 isValid := io.jwt.verify_es256(es256_token, jwks) 3}  The io.jwt.verify_xxx function simply checks to see if the signature on the token is valid. It returns a boolean indicating signature validity. It does not check the token claims. The downside of this function is that you must already know the algorithm used and then call the correct validation function.\n 1decode_example = {\u0026#34;header\u0026#34;: header, \u0026#34;payload\u0026#34;: payload, \u0026#34;signature\u0026#34;: signature} { 2 [header, payload, signature] := io.jwt.decode(es256_token) 3}  The io.jwt.decode() function does not validate the JWS. It base64 decodes the token and returns it\u0026rsquo;s constituent parts (header, payload and signature).\n 1decode_verify_example = { \u0026#34;isValid\u0026#34;: isValid, \u0026#34;header\u0026#34;: header, \u0026#34;payload\u0026#34;: payload } { 2 [isValid, header, payload] := io.jwt.decode_verify(es256_token, { \u0026#34;cert\u0026#34;: jwks, \u0026#34;iss\u0026#34;: \u0026#34;xxx\u0026#34;, }) 3}  The io.jwt.decode_verify() function detects the correct algorithm from the JWS headers, validates the signature and validates the claims in the token. The second parameter for the decode and verify function is an object that contains the validation parameters. The cert property must be a string. A JSON web token keyset must be serialized to a string if you have it already read into memory as a parsed object. The other properties in the object are the expected token values. If an audience claim is specified in the token, then an audience claim must be specified in the parameter list. The audience claim is also assumed to be an array of strings. Audience claim validation checks for the presence of the supplied audience string anywhere in the audience claim array in the JWS.\nThe REGO policy in this file decodes the token using all 3 functions above and assigns them to the result.\n 1result = { 2 \u0026#34;verify_example\u0026#34;: verify_example, 3 \u0026#34;decode_example\u0026#34;: decode_example, 4 \u0026#34;decode_verify_example\u0026#34;: decode_verify_example 5}  The output below shows the result of executing the policy.\n 1{ 2 \u0026#34;result\u0026#34;: [ 3 { 4 \u0026#34;expressions\u0026#34;: [ 5 { 6 \u0026#34;value\u0026#34;: { 7 \u0026#34;decode_example\u0026#34;: { 8 \u0026#34;header\u0026#34;: { \u0026#34;alg\u0026#34;: \u0026#34;ES256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; }, 9 \u0026#34;payload\u0026#34;: { \u0026#34;iss\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;nbf\u0026#34;: 1444478400 }, 10 \u0026#34;signature\u0026#34;: \u0026#34;940adccdf37ea482fca1453eecf53cdeefb37d7a2e8170598fa76b15e285b0f128561cbd580ca266546c858aa34d25dd6b0f32c362fea2fb78cd35196f63d632\u0026#34; 11 }, 12 \u0026#34;decode_verify_example\u0026#34;: { 13 \u0026#34;header\u0026#34;: { \u0026#34;alg\u0026#34;: \u0026#34;ES256\u0026#34;, \u0026#34;typ\u0026#34;: \u0026#34;JWT\u0026#34; }, 14 \u0026#34;isValid\u0026#34;: true, 15 \u0026#34;payload\u0026#34;: { \u0026#34;iss\u0026#34;: \u0026#34;xxx\u0026#34;, \u0026#34;nbf\u0026#34;: 1444478400 } 16 }, 17 \u0026#34;verify_example\u0026#34;: { 18 \u0026#34;isValid\u0026#34;: true 19 } 20 }, 21 \u0026#34;text\u0026#34;: \u0026#34;data.jws.examples.v1.result\u0026#34;, 22 \u0026#34;location\u0026#34;: { 23 \u0026#34;row\u0026#34;: 1, 24 \u0026#34;col\u0026#34;: 1 25 } 26 } 27 ] 28 } 29 ] 30}  The policy_examples directory contains several scripts and rego policies that demonstrates 5 different ways to invoke our policy:\n policy_examples/run_rego_tests_locally.sh - This script runs the test policies with a locally installed Open Policy Agent. As you can see from this example, the output looks much like outpu from test runners in most other languages. policy_examples/run_rego_tests_in_docker.sh - This script runs the test policies using the Open Policy Agent docker container and a locally mapped volume The nice part about using this approach is that it doesn\u0026rsquo;t require anything installed locally other than docker. policy_examples/run_opa_cli.sh - This script uses eval statements to test policies using the Open Policy Agent docker container. It shows the results of using all 3 JWS functions io.jwt.verify_es256(), io.jwt.decode() and io.jwt.decode_verify() REGO commands in jws_examples_v1.rego. Additionally it shows using eval statements with our final policy.rego. You should see an output similar to this for each request. As you can see from this example and the REST API example that follows, the output from the eval statement approach and the REST API approach are a bit different. The eval statement technique returns a result object and an array of expression evalution objects. Within each of those objects is a value object that finally contains our results. policy_examples/run_opa_rest_decision_api_tests.sh - uses Curl commands and OPA\u0026rsquo;s REST API running in docker. The REST API has both a decision ID and the results in a results object. The decision ID can be used to with Open Policy Agent\u0026rsquo;s decision logs to find out more information about how the result was arrived at. demonstrate_opa_jws_validation.sh - This script uses the final solution that shows our fully integrated solution with Envoy. This script uses Curl to make requests to our target service. Envoy asks OPA for an authorization decision prior to forwarding to HTTPBIN. In the final result you can see the details of the curl request going into Envoy and the decision is implied from being able to see the response from the requested API or a 403 Forbidden response is given.  Conclusion The difficult part of getting this solution to work is tinkering required to get the Envoy configuration correct and understanding the idiosyncrasies of each of the Open Policy agent JWS functions. The purpose of this article is to shortcut that process for you by giving you a known good starting point with a working configuration and rego policy. Once you have a functioning example, it is easy to adapt the example to whatever your local use cases might be.\nIn the next getting started guide, we are going to show how you can also validate JWS tokens in Envoy without using OPA at. This gives you a choice to make about where you would like that authorization decision to be made.\n",
    "ref": "/blog/envoy_opa_5_opa_jws/"
  },{
    "title": "Using the Open Policy Agent CLI",
    "date": "September 2, 2020",
    "description": "Learn how to use Open Policy Agent Command Line Interface",
    "body": "Getting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 04 \u0026mdash; Using the Open Policy Agent Command Line Interface This is the 4th Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 4 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Installing Open Policy Agent Locally Intalling Open Policy Agent (OPA) is pretty simple. It is a very small executable and is entirely self contained with no other files required. Depending on your operating system, the command to install OPA is slightly different:\n Download the executable:   Mac OSX: curl -L -o opa https://openpolicyagent.org/downloads/latest/opa_darwin_amd64 Linux : curl -L -o opa https://openpolicyagent.org/downloads/latest/opa_linux_amd64  Give OPA permission to execute chmod 755 ./opa Move OPA somewhere in your path mv opa /somewhere/in/path  Using the CLI The Open Policy Agent command line interface is used for one of 3 different purposes:\n Using the OPA in interactive mode to write and evaluate REGO statements with realtime feedback after each statement Running policies by supplying a file for the input and another file with the policy to execute Starting OPA as a service to use it\u0026rsquo;s API for decisioning  Read Evaluate Print Loop (REPL) \u0026amp; Interactive Playground Interactive mode is sometimes referred to as a read-evaluate-print-loop (REPL). OPA will wait and read in input from a user. It will then evaluate the input and print the results of the evaluation for the user to see. It then loops through these steps over again until one of the statements that needs to be evaluated is the exit statement.\n To start using the OPA interactively simply enter the command opa run When you are ready to leave just enter the command exit  The Open Policy Agent web site has a nice tutorial for getting started with the REPL and interactively writing REGO policies. So, we won\u0026rsquo;t repeat that here. It is located at https://www.openpolicyagent.org/docs/latest/. The latest version of the getting started is an interactive notebook. So, you can edit the commands right on the web page and see the results.\nThere is also an interactive playground located at https://play.openpolicyagent.org/ It can be used to accomplish the same thing that we will demonstrate below. However, you probably don\u0026rsquo;t want to use these tools when you are writing your own policies. The logic of your policies may be confidential and proprietary and the data used in the policies may be confidential as well.\nUsing the CLI to evaluate a policy with some test data The Rego Policy\nFor this getting started example we will use the same policy that we created for Getting Started #3. It simply checks the request method and allows the request through if the Method is a GET operation.\nAn Allowed Input\nWe will also need to simulate an input that we expect will be allowed through. We are using the same data structure that Envoy sends us. Since we are only looking at the request Method, line 38 is only property that we are concerned about.\nRun the command\nTo run the policy with the input that we just covered, we use the eval subcommand as part of the OPA cli. We then need to pass in a few parameters:\n -i specifies the name of the file that we want to use as input -d specifies the name of the file contains the REGO policy that we want to run The last parameter is the query that we want to run. This was implied for us in the previous lesson since it a default is already baked into the containers that we used. In the command line below, we see that the query is data.envoy.authz.allow  opa eval -i ./data/allowed_input.json -d ./policies/policy.rego 'data.envoy.authz.allow'` The supplied bash script can save you some typing and will also run both the allowed example and the denied example.\n  This script saves you from having to install OPA locally. It runs OPA in docker. ./demonstrate_opa_cli_in_docker.sh\n  This script runs OPA locally../demonstrate_local_opa_cli.sh\n  The results\nAfter the command runs, you should see something like the image below. OPA returns a JSON Document with a results object and an array of processing results for expressions.\nUsing the CLI to execute REGO Tests Thankfully Open Policy Agent has a built in unit testing capability. It is really easy to use and will give you confidence when you are managing a large set of rules and periodically make changes to them. Just name your test file the same name as your policy with _test added to the base filename. Test are written in Rego just like the policies. We need to specify the same package name package envoy.authz as the policy that we are testing. Each test is actually a rego rule. The rego rule name must begin with the word test. The rest of the name should be descriptive of the test and expected result. e.g. test_get_allowed\nThe body of the rule is of the form:\n rule to run in the policy under test with input as simulated test input  Here is the test file that we have written to test our policy:\nAs you can see we simply drop in Envoy\u0026rsquo;s JSON input that we are testing and the result we expect (either allowed or not allowed). The highlighted property in each test data set shows the only value that we are looking for in our rules:\n The allowed test case has a GET method in the test data The not allowed test case has a POST which falls into the the set of all methods other than GET are not allowed  Using the API Interactively to Develop Policies OPA policy agent also supports running as a service and accessing it via REST APIs. Detailed API information can be found on the Open Policy Agent web site. Here is a summary of the APIs.\nPolicy API for Creating, Updating or Deleting Policies\n* GET /v1/policies #--- List Policies * GET /v1/policies/\u0026amp;ltid\u0026amp;gt #--- Get a specific Policy * PUT /v1/policies/\u0026amp;ltid\u0026amp;gt #--- Create or Update a Policy * DELETE /v1/policies/\u0026amp;ltid\u0026amp;gt #--- Delete a policy Data API for Creating, Updating or Deleting Data and Getting Decisions\n* GET /v1/data/{path:.+} #--- Get a Document * POST /v1/data/{path:.+} #--- Get a Document (with Input e.g. get a decision). The input document `{ \u0026#34;input\u0026#34;: ... }` is passed in the request body * PUT /v1/data/{path:.+} #--- Create or Overwrite a Document * PATCH /v1/data/{path:.+} #--- Patch a Document * DELETE /v1/data/{path:.+} #--- Delete a Document Tips when using the API\n It is important to understand how the OPA document model works. The document model is described on the Open Policy Agent web site. Attempting to delete everything by supplying the root path for the policy or data endpoint doesn\u0026rsquo;t delete everything. This makes sense after thinking about it. It would be very easy to accidentally clear out an entire OPA engine and un-intentionally cause an outage if that was done in production. Even though all of the policies are visiable in the data endpoint when queried, they can\u0026rsquo;t be created, deleted or modified using that endpoint. It is mearly a way of invoking the policy and getting a decision. (see the policy in the highlighted section below)  Even though data and policies can be loaded in the same data document tree, it is confusing and not recommended (e.g. I deleted that data why is it still there? Oh, that\u0026rsquo;s because it is a policy and can\u0026rsquo;t be deleted by the data API):  /v1/data/{packagename}/{data document name} /v1/data/{packagename}/{policy rule name}   A clearer approach is to have a reference data and policy path like this:  /v1/data/reference/{data document path and name} /v1/data/app/{packagename}/{policy rule name}   The Policy API will store a policy with any provided key. However, it is loaded into the global data document based on its package name. Using the package name as the policy ID is an intuitive way to refer to policies. The policy ID field supports forward slashes. Be careful When assigning calculated or lookup values to a variable that you want to return in the result set. OPA will not return an undefined value in a property of a JSON document. An undefined value will fail the entire rule and cause OPA to return an empty object if no statically defined default is specified.  Producing Reason Codes with REGO Sometimes it is desirable for debugging or for audit or regulatory purposes to capture a reason code for decision results (usually denial reason but sometime approval too ). The OPA playground has a pet shop app example. It can be found in the REGO playground. Just select the attribute based access control example from the drop down menu.\nThe example below is modified to provide approval reasons and other messages in the case of a denial to help understand why access was denied. Logical OR conditions are expressed by creating multiple rules with the same name. The first 4 allow rules below are various reasons to approve the access request. If any of these rules is true the response is set to a static object with the approval decision and the reason code. The 5th allow rule is intended fire only when all of the approval rules fail. It is intended to give us insight into why the access request was denied. So this rule tests to see if all of the other rules fail.\nTo make sure the approval rules and deny rule don\u0026rsquo;t get out of sync, they both point to named approval rules below.\nThe messaging section is similar to the logical OR approval rules above. However they are structured a little bit differently. The square brackets enable these rules to return an array / collection of results. Unlike other data types where an undefined value will cause the rule to fail, it will return an empty array if none of the rules triggers the population of a message.\nWith these changes we now have insight into the output of the rules. In the example below, we send in a request with an un-registered user. If more insight is needed there is also the option to add the explain = full parameter to see a trace of the rule execution. The response field shows the results with provenance, explanation, metrics, and the messages that indicate which rules failed.\nThe images below zoom in on the explaination returned. The highlighted sections shows the step by step rule execution and where the rule failures occurred. On the left hand side is a reference to the exact line number in the rego file where the rule failure occurred. Any rule that results in an \u0026lsquo;undefined\u0026rsquo; state is considered a failure.\nExploring the OPA API with a sample Postman collection To make it easier to explore the OPA APIs, a postman collection pre-populated with the example data and policy is included with the rest of the code in the github repository.\nUploading the Pet Store App ABAC Policy via Postman\nUploading the Pet Store App Reference Data\nExploring the OPA API with a sample bash script If you don\u0026rsquo;t already use Postman, another option to explore the OPA APIs that does not require a large install is to use the included bash script. Simply run demonstrate_opa_api_in_docker.sh\nCongratulations! We have walked through how to use the OPA CLI, test REGO policies and use the OPA REST API. With the basics under your belt, we can move on to more sophisticated use cases such as Signed JSON Web Token (JWS) validation.\n",
    "ref": "/blog/envoy_opa_4_opa_cli/"
  },{
    "title": "Plugging Open Policy Agent into Envoy",
    "date": "September 1, 2020",
    "description": "Learn how to use Open Policy Agent with Envoy for more powerful authorization rules",
    "body": "Photo by Kutan Ural on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 03 \u0026mdash; Plugging OPA into Envoy This is the 3rd Envoy \u0026amp; Open Policy Agent Getting Started Guide. Each guide is intended to explore a single feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nAll of the source code for this getting started example is located on github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 3 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Envoy\u0026rsquo;s External Authorization API Envoy has the capability to call out to an external authorization service. There are 2 protocols supported. The authorization service can be either a RESTful API endpoint or using Envoy\u0026rsquo;s new gRPC protocol. Envoy sends all of the request details to the authorization service including the method, URI, parameters, headers, and request body. The authorization service simply needs to return 200 OK to permit the request to go through.\nIn this example we will be using a pre-built Open Policy Agent container that already understands Envoy\u0026rsquo;s authorization protocol.\nOpen Policy Agent Overview The Open Policy Agent site describes it very succinctly\nThe Open Policy Agent (OPA) is an open source, general-purpose policy engine. OPA provides a declarative language that lets you specify policy as code and APIs to offload policy decision-making from your software. OPA to enforce policies in microservices, Kubernetes, CI/CD pipelines, API gateways, or nearly any other software.\nOPA focuses exclusively on making policy decisions and not on policy enforcement. OPA pairs with Envoy for policy enforcement. OPA can run as:\n A standalone service accessible via an API A library that can be compiled into your code  It has an extremely flexible programming model:\nAdditionally, the company behind OPA, Styra, offers control plane products to author policies and manage a fleet of OPA instances.\nOPA decouples policy decision-making from policy enforcement. When your software needs to make policy decisions it queries OPA and supplies structured data (e.g., JSON) as input. OPA accepts arbitrary structured data as input.\nOPA generates policy decisions by evaluating the query input against policies and data. OPA expresses policies in a language called Rego. OPA has exploded in popularity and has become a Cloud Native Computing Foundation incubating project. Typical use cases are deciding:\n Which users can access which resources Which subnets egress traffic is allowed Which clusters a workload can be deployed to Which registries binaries can be downloaded from Which OS capabilities a container can execute with The time of day the system can be accessed  Policy decisions are not limited to simple yes/no or allow/deny answers. Policies can generate any arbitrary structured data as output.\nThis getting started example is based on this OPA tutorial using docker-compose instead of Kubernetes.\nSolution Overview In this getting started example we take the super simple envoy environment we created in getting started episode 1 and add the simplest possible authorization rule using Open Policy Agent.\nDocker Compose This is the same docker compose file as our initial getting started example with a few modifications. At line 15 in our docker-compose file, we added a reference to our Open Policy Agent container. This is a special verison of the Open Policy Agent containers on dockerhub. This version is designed to integrate with Envoy and has an API exposed the complies with Envoy\u0026rsquo;s gRPC specification. There are also some other things to take notice of:\n Debug level logging is set on line 21 The gRPC service for Envoy is configured on line 23 The logs are sent to the console for a log aggregation solution to pickup (or not) on line 24  Envoy Config Changes There are a few things that we need to add to the envoy configuration to enable external authorization:\n We added an external authorization configuration at line 23. failure_mode_allow on line 26 determines whether envoy fails open or closed when the authorization service fails. with_request_body determines if the request body is sent to the authorization service. max_request_bytes determines how large of a request body Envoy will permit. allow_partial_message determines if a partial body is sent or not when a buffer maximum is reached. The grpc_service object on line 30 specifies how to reach the Open Policy Agent endpoint to make an authorization decision.  Rego: OPA\u0026rsquo;s Policy Language There are a lot of other examples of how to write Rego policies on other sites. This walkthrough of Rego is targeted specifically to using it to make Envoy decisions:\n The package statement on the first line determines where the policy is located in OPA. When a policy decision is requested, the package name is used as the prefix used to locate the named decision. The import statement on line 3 navigates the heavily nested data structure that Envoy sends us and gives us a shorter alias to refer to a particular section of the input. On line 5 we have a rule named allow and we set it\u0026rsquo;s default value to and object that Envoy is expecting.  The allowed property determines if the request is permitted to go through or not. The headers property allows us to set headers on either the forwarded request (if approved) or on the rejected request (if denied). OPA does not marshal any REGO data types on your behalf. All header values must be specified as a string. The body property can be used to communicate error message details to the requestor. It has no affect on requests approved for forwarding. OPA does not marshal any REGO data types on your behalf. The body value must be specified as a string. The http_status property can be set on any rejected requests to any value as desired   The next section starting at line 12 specifies the logic for approving the request to move forward  On line 12 the allow rule is set to the value of the response variable that we define in the body of the rule Line 13 is the only condition we have defined for approval of the request. The request method simply needs to be a POST. If true then we set the response to the values on lines 15 and 16.    Input data sent from Envoy Now we dive into the details of the input data structure sent from Envoy.\n Line 3 is an object that describes the IP address and Port that the request is going to Line 10 is the request object itself. The unmarshalled body is present along with:  request headers hostname where the original request was sent a unique request ID assigned by Envoy request method unparsed path protocol request size   Line 34 is a object that describes the IP address and port where the request originated Line 45 is where Envoy has already done some work for us to parse the request body (if it was configured to be forwarded) Line 46 and 47 are the parsed path and query parameters respectively Finally, line 48 let\u0026rsquo;s us know whether we have the complete request body or not  Reviewing the Request Flow in Envoy\u0026rsquo;s Logs In debug mode, we have a lot of rich information that Envoy logs for us to help us determine what may be going wrong as we develop our system.\n Line 1 shows our request first coming in The next several lines show us the request headers Line 13-14 let\u0026rsquo;s us know that envoy is buffering the request until it reaches the buffer limit that we set Line 16-20 show us that Envoy forwarded the request to our authorization service and got an approval to forward the request The remainder of the logs show envoy forwarding the request to its destination and then back to the calling client  Reviewing OPA\u0026rsquo;s Decision Log The open policy agent decision logs include the input that we just reviewed and some other information that we can use for debugging, troubleshoot or audit logging. The interesting parts that we haven\u0026rsquo;t reviewed yet:\n The decision ID on line 2 can be matched against other OPA log entries related to this decision Line 5 - Envoy sends us the destination of the request Line 7 - Envoy sends the request details. The unmarshalled body is only available if we setup the buffering and forwarding in the Envoy configuration. line 30 - Envoy also sends us the source IP address that it received the request from. Line 37 has the labels that show which Envoy instance the request came from Line 42 is an object that contains the performance metrics for the decision Line 50 shows the response that OPA sent back to Envoy  Taking this solution for a spin Now that we know what we are looking at, we can run this example by executing the script ./demonstrate_opa_integration.sh\n The script starts the envoy front proxy, HTTPBIN and Open Policy Agent Next it shows the docker container status to make sure everything is up and running. Curl is used to issue a request that should get approved, forwarded to HTTPBin and the display the results The opa decision logs are then shown to let you explore the information available for development debugging and troubleshooting To make sure we show both outcomes the next request should fail our simple authorization rule check. The decision logs are shown again Finally the example is brought down and cleaned up.  In the next getting started guide we will explore Open Policy Agent\u0026rsquo;s command line interface and unit testing support.\n",
    "ref": "/blog/envoy_opa_3_adding_open_policy_agent/"
  },{
    "title": "Adding Observability Tools",
    "date": "September 1, 2020",
    "description": "Learn how to add ElasticSearch and Kibana to your Envoy front proxy environment",
    "body": "Photo by Alex Eckermann on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 02 \u0026mdash; Adding Log Aggregation to our Envoy Example This is the 2nd Envoy \u0026amp; Open Policy Agent (OPA) Getting Started Guide. Each guide is intended to explore a single Envoy or OPA feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the goal of creating a very powerful authorization service by the end of the series.\nWhile our solution is still very simple, it is a great time to show how to make our solution observable with log aggregation. This makes it easier to think about how to scale and productionize our solution. As we start to develop and apply authorization rules at scale it will be handy to have all of the logs aggregated and displayed in one place for development and troubleshooting activies. In this article we will walk through how to setup the EFK stack to pull your logs together from all of the docker containers in your local development environment.\nAll of the source code for this getting started example is located on github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 2 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Solution Overview The solution that we will build in this blog is shown below. We will send docker logs into an EFK stack that is also running inside docker. Each of the containers in our solution simply send logs to Standard out and / or Standard Error. No agents nor other special software is requirements are imposed on the observered applications.\nAdding EFK containers We will be using Fluent Bit in this example because it is lite weight and simpler to deal with than Logstash or full fledged FluentD. Below is a very basic configuration with no special optimizations. Lines 18 - 49 add the EFK stack to our environment. Lines 16 and 47 use the depend_on property to cause docker to start elasticSearch first and then Kiban and Fluent Bit that depend on elasticSearch.\nWiring our containers into EFK With the EFK log aggregation containers added to our docker-compose file, we now need to wire them into the other containers in our environment. The changes below show a couple of small changes that we needed to make to our compose file from where we left off in Getting Started Guide #1. We added the property at line 14 below which expresses our dependency on elasticSeach. Additionally, we need to wire standard out and standard error from our containers to Fluent Bit. This is done through the logging properties on lines 17 and 27. The driver line tells docker which log driver to use and the tag help make it more clear which container is the source of the logs.\nTaking things for a spin The demonstration script spins everything up for us. Just run ./demonstrate_front_proxy.sh to get things going:\n It downloads and spins up all of our containers. Then it waits 30 seconds to give elasticSearch some time to get ready and some time for Kibana to know that elasticSearch is ready. A curl command sends Envoy a request to make sure the end-to-end flow is working. If that worked, proceed forward. If not wait a bit longer to make sure elasticSearch and Kibana are both ready. If you are running on Mac OS X then the next step will open a browser and take you to the page to setup your Kibana index. If it doesn\u0026rsquo;t work, simply open your browser and go to http://localhost:5601/app/kibana#/management/kibana/index_pattern?_g=() you should see something like this:  I simply used log* as my index and clicked next. Which should bring up a screen to select the timestamp field name. Select @timestamp and click create index. You should see some field information about your newly created index.  The script then uses the Open command to navigate to the log search interface. If it doesn\u0026rsquo;t work on your operating system then simply navigate to http://localhost:5601/app/kibana#/discover. You should see something like this with some log results already coming in. If you have an interest, you may want to select the container_name and log columns to make it easier to read through the debug logs and results of your testing efforts.  The script sends another request through envoy and you should be able to see the logs coming into EFK.  The script with then take down the environment.  In the next getting started guide, we will add in Open Policy Agent and begin experimenting with a simple authorization rule.\n",
    "ref": "/blog/envoy_opa_2_adding_observability/"
  },{
    "title": "Using Envoy as a Front Proxy",
    "date": "August 31, 2020",
    "description": "Learn how to set up Envoy as a front proxy with docker",
    "body": "Photo by Mattia Serrani on Unsplash\nGetting Started with Envoy \u0026amp; Open Policy Agent \u0026mdash; 01 \u0026mdash; Using Envoy as a Front Proxy This is the 1st Envoy \u0026amp; Open Policy Agent (OPA) Getting Started Guide. Each guide is intended to explore a single Envoy or OPA feature and walk through a simple implementation. Each guide builds on the concepts explored in the previous guide with the end goal of building a very powerful authorization service by the end of the series.\nThe source code for this getting started example is located on Github. \u0026mdash;\u0026mdash;\u0026gt; Envoy \u0026amp; OPA GS # 1 \nHere is a list of the Getting Started Guides that are currently available.\nGetting Started Guides  Using Envoy as a Front Proxy Adding Observability Tools Plugging Open Policy Agent into Envoy Using the Open Policy Agent CLI JWS Token Validation with OPA JWS Token Validation with Envoy Putting It All Together with Composite Authorization Configuring Envoy Logs Taps and Traces Sign / Verify HTTP Requests  Overview Envoy is an open source edge and service proxy that has become extremely popular as the backbone underneath most of the leading service mesh products (both open source and commercial). This article is intended to demystify it a bit and help people understand how to use it on it\u0026rsquo;s own in a minimalist fashion.\nEnvoy is just like any other proxy. It receives requests and forwards them to services that are located behind it. The 2 ways to deploy Envoy are:\n  Front Proxy - In a front proxy deployment Envoy is very similar to NGINX, HAProxy, or an Apache web server. The Envoy server has it\u0026rsquo;s own IP address and is a separate server on the network from the services that it protects. Traffic comes in and get forwarded to a number of different services that are located behind it. Envoy supports a variety of methods for making routing decisions.\n One mechanism is to use path based routing to determine the service of interest. For instance a request coming in as: /service1/some/other/stuff can use the first uri path element as a routing key. service1 can be used to route requests to service 1. Additionally, service2 in /service2/my/applications/path can be used to route the request to a set of servers that support service 2. The path can be rewritten as it goes through Envoy to trim off the service routing prefix. Another mechanism is to use Server Name Indication (SNI) which is a TLS extension to determine where to forward a request. Using this technique, service1.com/some/other/stuff would use the server name to route to the service1 services. Additionally, service2.com/my/applications/path would use service2.com to route the request to service2. The diagram above shows the front proxy approach.    Sidecar Proxy - In a sidecar deployment, the Envoy server is located at the same IP address as each service that it protects. The Envoy server when deployed as as sidecar only has a single service instance behind it. The sidecar approach can intercept all inbound traffic and optionally all outbound traffic on behalf of the service instance. IP Tables rules are typically used to configure the operating system to capture and redirect this traffic to Envoy. The diagram above shows the sidecar approach.\n  In this article and example project we will start with the simplest possible Envoy deployment. This example just uses docker compose to show how to get Envoy up and running. There will be a number of subsequent articles that expand on this simple approach to demostrate more Envoy capabilies. Open Policy Agent will also be introduced to handle more complex authorization use cases that cannot be handled by Envoy alone.\nThe diagram below shows the environment that we are about to build and deploy locally.\nBuilding an Envoy Front Proxy The code for the complete working example can be found on Github. We will start with the Envoy docker images. The Envoy images are located on Dockerhub. We will use docker-compose to build some configurability into our Envoy environment.\nDockerfile The entrypoint.sh file is where the magic happens. We will configure environment variables in our docker-compose file to determine which service (SERVICE_NAME) Envoy routes to and the port (SERVICE_PORT) on that service. Additionally, we specify how much detail is captured in the logs by setting the DEBUG_LEVEL environment variable. As you can see from the script below on line 3, we replace those environment variables on the fly in Envoy\u0026rsquo;s configuration file before starting Envoy.\nentrypoint.sh Since we don\u0026rsquo;t have a configuration file yet, we will cover that next. Envoy is very flexible and powerful. There is an enormous amount of expressiveness that the Envoy API and configuration files support. With this flexibility and power, Envoy configuration files can become quite complicated with a lot layers in the YAML hierarchy. Additionally, each feature has a lot of configuration parameters. The documentation can only cover so much of that functionality with an open source community of volunteers.\nOne of the challenges that I have when reading through the documentation and trying to apply it, is that the documentation has a variety of YAML snippets. There are very few places that these YAML snippets are pulled together into a functioning example. There are a few examples in the source code examples directory but they are far from comprehensive. That leaves a lot of tinkering for engineers to figure out how to compose a functional configuration while interpretting sometimes unclear error messages on their way to the promised land. That is the entire reason that I am writting a series of getting started guides. These articles are intended to give folks a known to work starting point for Envoy authorization features and extensions like Open Policy Agent.\nThe Envoy configuration starts with defining a listener as we can see starting on line 3. The first property is the address and port to accept traffic on (lines 3 through 6). The next property is a filter chain. Filter chains are very powerful and enable configuration for a wide variety of possible behaviors. This filter chain is as simple as it gets. It simply accepts any HTTP traffic with any URI pattern and routes it to the cluster named service.\nThe http_connection_manager component does this for us. It\u0026rsquo;s configuration starts on line 9 and extends to line 24. Execution order is determined by the order they are listed in the configuration file. The important part for this discussion begins on line 14 with the route_config. This sets up routing requests for any domain (line 18) and any request URI that begins with a slash (line 20) to go to the cluster named service. The cluster definitions are in a separate section to make them reusable destination across a variety of rules.\nEnvoy Configuration (envoy.yaml) The cluster definitions begin on line 25. We can see that there is only a single cluster defined. It has the name service, uses DNS to find server instances and uses round robin to direct traffic across multiple instances. The hostname is on line 32 and the port is on line 33. As we can see these are the environment variables that we will swap out with the entry.sh script.\nThe last section of the configuration file tells Envoy where to listen for admin traffic. The admin gui is a handy little tool that we will not cover in this guide but is definitely worth poking around in to observe what is going on inside an individual Envoy instance.\nDocker Compose Configuration Now that we understand the Envoy configuration we can move on to understanding the rest of the simple environment that we are setting up. Line 4 shows the trigger that causes docker to build the Envoy container. Docker will only build the Envoy Dockerfile the first time it sees that an image does not exist. If you want to force rebuilding the Envoy container on subsequent runs add the --build parameter to your docker compose command. We expose Envoy to the host network on lines 6 and 7 and provide the configuration file that we just created on line 9.\nThe service to route to and port are defined on the environment variables on lines 12 and 13. Notice the name app matches the name of our final service on line 15. We are simply using HTTPBIN to reflect our request back to us.\nRunning and Trying out our Example The last step to getting our front proxy up is simply running the included script test.sh that demonstrates our example. The script explains what it is about to do to ensure you know what are about to see scrolling across your terminal screen. Line 3 starts our environment. Line 8 let\u0026rsquo;s you check to make sure both containers are running before trying to send them a request.\nLine 10 simply calls Envoy with a curl command with the --verbose parameter set so that you can see the headers and request details. Then line 12 tears down the whole environment.\nContainers UP!!!! If you have successfully started your environment then you should see something like this:\nWe Succeeded!!! You should see something like this if you successfully called HTTPBin through Envoy:\nCongratulations Congratulations, you have successfully stood up your first Envoy instance and configured it to forward traffic! This is the simplest possible Envoy configuration :) We don\u0026rsquo;t have any security yet or any other features that Envoy is famous for. We will get to that in future articles. Feel free to use postman to explore other requests that you can send. Additionally, don\u0026rsquo;t forget to explore Envoy\u0026rsquo;s admin console by pointing your web browser to http://localhost:8001\n",
    "ref": "/blog/envoy_opa_1_front_proxy/"
  },{
    "title": "About",
    "date": "February 28, 2019",
    "description": "Helpful Badger, searching the world for helpful tech tips",
    "body": "Photo by Vincent van Zalinge on Unsplash\nThe Helpful Badger loves technology. He travels the world looking for interesting technology topics and shares his perspective on what he finds.\nIf you see him out it the woods seeking his next adventure, offer him a StarBucks Pike Place coffee. He\u0026rsquo;ll gladly pull up a chair and talk tech with you!\n",
    "ref": "/about/"
  }]
